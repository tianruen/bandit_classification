{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import sklearn.metrics as sk_metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 2, 'name': 'Adult', 'repository_url': 'https://archive.ics.uci.edu/dataset/2/adult', 'data_url': 'https://archive.ics.uci.edu/static/public/2/data.csv', 'abstract': 'Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset. ', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 48842, 'num_features': 14, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'], 'target_col': ['income'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1996, 'last_updated': 'Mon Aug 07 2023', 'dataset_doi': '10.24432/C5XW20', 'creators': ['Barry Becker', 'Ronny Kohavi'], 'intro_paper': None, 'additional_info': {'summary': 'Extraction was done by Barry Becker from the 1994 Census database.  A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\r\\n\\r\\nPrediction task is to determine whether a person makes over 50K a year.\\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.', 'citation': None}}\n",
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "X = X.drop([\"education\"], axis=1)   # similar columns exist in the dataset, which is education-num\n",
    "X = X.dropna()\n",
    "\n",
    "y = adult.data.targets \n",
    "y = y.iloc[X.index]\n",
    "\n",
    "# metadata \n",
    "print(adult.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(adult.variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reward for each class label.\n",
      "\t- Class 0 : 0.2770\n",
      "\t- Class 1 : 0.4070\n",
      "\t- Class 2 : 0.4937\n",
      "\t- Class 3 : 0.7169\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=25)\n",
    "\n",
    "cat_var = ['workclass', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "X_train = pd.get_dummies(data=X_train, columns=cat_var, drop_first=True)\n",
    "X_test = pd.get_dummies(data=X_test, columns=cat_var, drop_first=True)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=False)\n",
    "\n",
    "num_var = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "scaler = StandardScaler()\n",
    "scaler_fit = scaler.fit(X_train[num_var])\n",
    "X_train_ = pd.DataFrame(scaler_fit.transform(X_train[num_var]))\n",
    "X_test_ = pd.DataFrame(scaler_fit.transform(X_test[num_var]))\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "X_train_.columns = num_var\n",
    "X_test_.columns = num_var\n",
    "X_train[num_var] = X_train_[num_var]\n",
    "X_test[num_var] = X_test_[num_var]\n",
    "\n",
    "y_train[\"income\"] = pd.Categorical(y_train[\"income\"]).codes\n",
    "y_test[\"income\"] = pd.Categorical(y_test[\"income\"]).codes\n",
    "\n",
    "X_train, X_test, y_train, y_test = X_train.to_numpy().astype(float), X_test.to_numpy().astype(float), y_train.to_numpy().reshape(-1), y_test.to_numpy().reshape(-1)\n",
    "\n",
    "label, num_class = np.unique(y_train, return_counts=True)\n",
    "rewards = 1 / num_class**(1/2)\n",
    "rewards_lab = np.round(rewards / np.linalg.norm(rewards), 4)\n",
    "print(\"\\nReward for each class label.\")\n",
    "for idx, reward in enumerate(rewards_lab):\n",
    "    print(\"\\t- Class {} : {:.4f}\".format(idx, reward))\n",
    "    \n",
    "minority = np.argmin(num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0, proportion 0.5188996220075599\n",
      "class 1, proportion 0.24029519409611808\n",
      "class 2, proportion 0.1633467330653387\n",
      "class 3, proportion 0.07745845083098338\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(num_class)):\n",
    "    print(f\"class {i}, proportion {num_class[i]/np.sum(num_class)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, num_class = np.unique(y_train, return_counts=True)\n",
    "rewards = 1 / num_class**(1/2)\n",
    "rewards_lab = np.round(rewards / np.linalg.norm(rewards), 4)\n",
    "\n",
    "def get_reward(actual, pred):\n",
    "    r = rewards_lab[actual].item()\n",
    "    return r if pred == actual else -r\n",
    "\n",
    "def get_reward_batch(actual, pred):\n",
    "    rewards_table = torch.tensor(rewards_lab, device=actual.device)\n",
    "    reward = rewards_table.gather(0, actual.to(torch.int64))\n",
    "    return torch.where(actual == pred, reward, -reward)\n",
    "\n",
    "def macro_avg_precision(y_pred, y_true):\n",
    "    '''\n",
    "    Calculate precision for each class and take the average\n",
    "    '''\n",
    "    eps = 1e-10\n",
    "    assert type(y_pred) == type(y_true)\n",
    "    if type(y_pred) != torch.Tensor:\n",
    "        y_pred = torch.tensor(y_pred)\n",
    "        y_true = torch.tensor(y_true)\n",
    "        \n",
    "    num_classes = len(torch.unique(y_true))\n",
    "    precision = 0\n",
    "    \n",
    "    for c in torch.unique(y_true):\n",
    "        temp_tp = torch.where((y_pred == c) & (y_true == c), 1, 0).sum()\n",
    "        temp_fp = torch.where((y_pred == c) & (y_true != c), 1, 0).sum()\n",
    "        \n",
    "        temp_precision = temp_tp / (temp_tp + temp_fp + eps)\n",
    "        \n",
    "        precision += temp_precision\n",
    "    \n",
    "    return precision / num_classes\n",
    "\n",
    "def micro_avg_precision(y_pred, y_true):\n",
    "    '''\n",
    "    Calculate class wise TP and FP, then use it to calculate overall precision\n",
    "    '''\n",
    "    eps = 1e-10\n",
    "    assert type(y_pred) == type(y_true)\n",
    "    if type(y_pred) != torch.Tensor:\n",
    "        y_pred = torch.tensor(y_pred)\n",
    "        y_true = torch.tensor(y_true)\n",
    "    \n",
    "    num_classes = len(torch.unique(y_true))\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for c in torch.unique(y_true):\n",
    "        tp += torch.where((y_pred == c) & (y_true == c), 1, 0).sum()\n",
    "        fp += torch.where((y_pred == c) & (y_true != c), 1, 0).sum()\n",
    "    \n",
    "    precision = tp / (tp+fp+eps)\n",
    "    return precision\n",
    "\n",
    "def pred_minority_summary(y_truth, y_pred):\n",
    "    assert type(y_pred) == type(y_truth)\n",
    "    if type(y_pred) != torch.Tensor:\n",
    "        y_pred = torch.tensor(y_pred)\n",
    "        y_truth = torch.tensor(y_truth)\n",
    "        \n",
    "    actual_minority = torch.bincount(y_truth.reshape(1,-1)[0])[1:].sum().item()\n",
    "    pred_count_dist = torch.bincount(y_pred.reshape(1,-1)[0])\n",
    "\n",
    "    if len(pred_count_dist) > 1:\n",
    "        pred_minority = pred_count_dist[1:].sum().item()\n",
    "    else:\n",
    "        # uniq_element = pred_count_dist.unique().item()\n",
    "        pred_minority = 0\n",
    "        uniq_element = y_pred.unique().item()\n",
    "        print(f\"Only class {uniq_element} is predicted\")\n",
    "\n",
    "    pred_correct = torch.where((y_pred == y_truth) & (y_truth!=0), 1, 0).sum().item()\n",
    "    \n",
    "    return (actual_minority, pred_minority, pred_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralEPSG(nn.Module):\n",
    "    def __init__(self, num_arms, context_dim, hidden_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.num_arms = num_arms\n",
    "        self.context_dim = context_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.training_step = 0\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(context_dim + num_arms - 1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        \n",
    "    def forward(self, context):\n",
    "        return self.model(context)\n",
    "    \n",
    "    def cal_reward(self, x):\n",
    "        device = next(self.model.parameters()).device\n",
    "        dtype = next(self.model.parameters()).dtype\n",
    "        \n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        bs = x.shape[0]\n",
    "        est_reward = []\n",
    "        \n",
    "        for a in range(self.num_arms):\n",
    "            context = torch.zeros(bs, self.num_arms, device=device)\n",
    "            context = torch.cat([context[:,:a].reshape(bs,-1), x, context[:,a+1:].reshape(bs,-1)], dim=1).to(dtype)\n",
    "            pred = self.model(context)\n",
    "            est_reward.append(pred)\n",
    "            \n",
    "        est_reward = torch.cat(est_reward, dim=1)\n",
    "        return est_reward\n",
    "\n",
    "    def select_arm(self, x):\n",
    "        device = next(self.model.parameters()).device\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "        epsilon = torch.clamp(\n",
    "            torch.tensor([(0.01 - 1) / 10000 * self.training_step + 1]),\n",
    "            min=0.001,\n",
    "            max=1\n",
    "        )\n",
    "        \n",
    "        self.training_step += bs\n",
    "        \n",
    "        if torch.rand(1) < epsilon:\n",
    "            # action = torch.randint(0, self.num_arms+1, (bs,))\n",
    "            return torch.randint(0, self.num_arms, (bs,), device=device)\n",
    "        else:\n",
    "            est_reward = self.cal_reward(x)\n",
    "            return torch.argmax(est_reward, dim=1).detach()\n",
    "        \n",
    "    def update(self, x, chosen_arm, reward):\n",
    "        device = next(self.model.parameters()).device\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        bs = x.shape[0]\n",
    "        zeros = torch.zeros(bs, self.context_dim + self.num_arms - 1, device=device)\n",
    "        chosen_arm = chosen_arm.unsqueeze(1).repeat(1, self.context_dim) + torch.arange(self.context_dim).unsqueeze(0).to(device)\n",
    "        context = zeros.scatter(1, chosen_arm, x)\n",
    "        \n",
    "        # pred_reward = self.forward(context)[chosen_arm]\n",
    "        pred_reward = self.model(context)\n",
    "        \n",
    "        if type(reward) != torch.Tensor:\n",
    "            device = next(self.model.parameters()).device\n",
    "            dtype = next(self.model.parameters()).dtype\n",
    "            reward = torch.tensor(reward, dtype=dtype, device=device).reshape(-1,1)\n",
    "        if len(reward.shape) == 1:\n",
    "            reward = reward.reshape(-1,1)\n",
    "        loss = nn.MSELoss()(pred_reward, reward.to(torch.float))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:17<00:00,  3.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "hidden_dim = 2048\n",
    "n_actions = len(np.unique(y_train))\n",
    "\n",
    "epsg = NeuralEPSG(num_arms=n_actions, context_dim=n_features, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "features_tensor = torch.tensor(X_train, dtype=torch.float, device=device)\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.float, device=device)\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "\n",
    "recall_list, precision_list, f_score_list, g_mean_list, epr_list = [], [], [], [], []\n",
    "recall_list_val, precision_list_val, f_score_list_val, g_mean_list_val, epr_list_val = [], [], [], [], []\n",
    "best_epr = 0\n",
    "\n",
    "for epoch in tqdm(range(5)):\n",
    "    epsg.training_step = 0\n",
    "    for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "        action = epsg.select_arm(features)\n",
    "        reward = get_reward_batch(labels, action)\n",
    "        epsg.update(features, action, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 981/981 [00:02<00:00, 390.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "Actual minority: 16037; Predicted minority: 8109; Correctly predicted: 3732\n",
      "Macro Average Precision: 0.3935583531856537; Micro Average Precision: 0.5781184434890747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = epsg\n",
    "model.eval()\n",
    "\n",
    "x = X_train\n",
    "y_truth = torch.tensor(y_train)\n",
    "\n",
    "n_sep = 1000\n",
    "sep = math.ceil(x.shape[0] / n_sep)\n",
    "y_pred = torch.tensor([], dtype=torch.int)\n",
    "# y_proba = torch.tensor([])\n",
    "\n",
    "for i in tqdm(range(math.ceil(x.shape[0]/sep))):\n",
    "    features = torch.tensor(x[i*sep:(i+1)*sep], dtype=torch.float, device=device)\n",
    "    cur_y_pred  = model.select_arm(features).cpu()\n",
    "    # cur_y_proba = nn.Softmax(dim=1)(model.cal_reward(features)).detach()[:,1].cpu()\n",
    "    \n",
    "    y_pred = torch.cat([y_pred, cur_y_pred], dim=0)\n",
    "    # y_proba = torch.cat([y_proba, cur_y_proba], dim=0)\n",
    "    \n",
    "    del features\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "minority_summary = pred_minority_summary(y_truth, y_pred)\n",
    "macro_avg_pre = macro_avg_precision(y_pred, y_truth)\n",
    "micro_avg_pre = micro_avg_precision(y_pred, y_truth)\n",
    "print(\"Training Data\")\n",
    "print(f\"Actual minority: {minority_summary[0]}; Predicted minority: {minority_summary[1]}; Correctly predicted: {minority_summary[2]}\")\n",
    "print(f\"Macro Average Precision: {macro_avg_pre}; Micro Average Precision: {micro_avg_pre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 953/953 [00:01<00:00, 660.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data\n",
      "Actual minority: 6864; Predicted minority: 3553; Correctly predicted: 1636\n",
      "Macro Average Precision: 0.33944010734558105; Micro Average Precision: 0.5798978209495544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = epsg\n",
    "model.eval()\n",
    "\n",
    "x = X_test\n",
    "y_truth = torch.tensor(y_test)\n",
    "\n",
    "n_sep = 1000\n",
    "sep = math.ceil(x.shape[0] / n_sep)\n",
    "y_pred = torch.tensor([], dtype=torch.int)\n",
    "# y_proba = torch.tensor([])\n",
    "\n",
    "for i in tqdm(range(math.ceil(x.shape[0]/sep))):\n",
    "    features = torch.tensor(x[i*sep:(i+1)*sep], dtype=torch.float, device=device)\n",
    "    cur_y_pred  = model.select_arm(features).cpu()\n",
    "    # cur_y_proba = nn.Softmax(dim=1)(model.cal_reward(features)).detach()[:,1].cpu()\n",
    "    \n",
    "    y_pred = torch.cat([y_pred, cur_y_pred], dim=0)\n",
    "    # y_proba = torch.cat([y_proba, cur_y_proba], dim=0)\n",
    "    \n",
    "    del features\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "minority_summary = pred_minority_summary(y_truth, y_pred)\n",
    "macro_avg_pre = macro_avg_precision(y_pred, y_truth)\n",
    "micro_avg_pre = micro_avg_precision(y_pred, y_truth)\n",
    "print(\"Test Data\")\n",
    "print(f\"Actual minority: {minority_summary[0]}; Predicted minority: {minority_summary[1]}; Correctly predicted: {minority_summary[2]}\")\n",
    "print(f\"Macro Average Precision: {macro_avg_pre}; Micro Average Precision: {micro_avg_pre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "Actual minority: 16037; Predicted minority: 7704; Correctly predicted: 5190\n",
      "Macro Average Precision: 0.7827357053756714; Micro Average Precision: 0.6513170003890991\n",
      "Test Data\n",
      "Actual minority: 6864; Predicted minority: 3166; Correctly predicted: 1607\n",
      "Macro Average Precision: 0.4344041049480438; Micro Average Precision: 0.5924266576766968\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(tree_method=\"hist\", enable_categorical=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "y_truth = y_train\n",
    "\n",
    "minority_summary = pred_minority_summary(y_truth, y_pred)\n",
    "macro_avg_pre = macro_avg_precision(y_pred, y_truth)\n",
    "micro_avg_pre = micro_avg_precision(y_pred, y_truth)\n",
    "print(\"Training Data\")\n",
    "print(f\"Actual minority: {minority_summary[0]}; Predicted minority: {minority_summary[1]}; Correctly predicted: {minority_summary[2]}\")\n",
    "print(f\"Macro Average Precision: {macro_avg_pre}; Micro Average Precision: {micro_avg_pre}\")\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_truth = y_test\n",
    "\n",
    "minority_summary = pred_minority_summary(y_truth, y_pred)\n",
    "macro_avg_pre = macro_avg_precision(y_pred, y_truth)\n",
    "micro_avg_pre = micro_avg_precision(y_pred, y_truth)\n",
    "print(\"Test Data\")\n",
    "print(f\"Actual minority: {minority_summary[0]}; Predicted minority: {minority_summary[1]}; Correctly predicted: {minority_summary[2]}\")\n",
    "print(f\"Macro Average Precision: {macro_avg_pre}; Micro Average Precision: {micro_avg_pre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "Actual minority: 16037; Predicted minority: 4412; Correctly predicted: 2601\n",
      "Macro Average Precision: 0.29342150688171387; Micro Average Precision: 0.584868311882019\n",
      "Training Data\n",
      "Actual minority: 6864; Predicted minority: 1911; Correctly predicted: 1128\n",
      "Macro Average Precision: 0.5440467000007629; Micro Average Precision: 0.5866872072219849\n"
     ]
    }
   ],
   "source": [
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\n",
    "    'learning_rate': 0.1,\n",
    "    'num_class': len(np.unique(y_train)),\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "num_round = 10\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[train_data])\n",
    "\n",
    "y_pred = np.argmax(bst.predict(X_train), axis=1)\n",
    "y_truth = y_train\n",
    "\n",
    "minority_summary = pred_minority_summary(y_truth, y_pred)\n",
    "macro_avg_pre = macro_avg_precision(y_pred, y_truth)\n",
    "micro_avg_pre = micro_avg_precision(y_pred, y_truth)\n",
    "print(\"Training Data\")\n",
    "print(f\"Actual minority: {minority_summary[0]}; Predicted minority: {minority_summary[1]}; Correctly predicted: {minority_summary[2]}\")\n",
    "print(f\"Macro Average Precision: {macro_avg_pre}; Micro Average Precision: {micro_avg_pre}\")\n",
    "\n",
    "y_pred = np.argmax(bst.predict(X_test), axis=1)\n",
    "y_truth = y_test\n",
    "\n",
    "minority_summary = pred_minority_summary(y_truth, y_pred)\n",
    "macro_avg_pre = macro_avg_precision(y_pred, y_truth)\n",
    "micro_avg_pre = micro_avg_precision(y_pred, y_truth)\n",
    "print(\"Training Data\")\n",
    "print(f\"Actual minority: {minority_summary[0]}; Predicted minority: {minority_summary[1]}; Correctly predicted: {minority_summary[2]}\")\n",
    "print(f\"Macro Average Precision: {macro_avg_pre}; Micro Average Precision: {micro_avg_pre}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
